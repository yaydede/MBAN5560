---
title: "Dummy Variables and Probability Models Assignment - SOLUTIONS"
subtitle: "Predicting High-Price Homes in Ames, Iowa"
author: "MBAN5520 - Statistics"
date: today
format: 
  html:
    embed-resources: true
    code-background: true
    toc: true
    toc-depth: 3
    code-fold: false
    theme: cosmo
    fig-width: 9
    fig-height: 6
    number-sections: true
execute:
  echo: true
  warning: false
  message: false
---

```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggplot2)
library(broom)
library(patchwork)

# Set seed for reproducibility
set.seed(5520)
```

# Introduction

## Assignment Objectives

This assignment applies dummy variable encoding and probability models to predict high-price homes using the Ames Housing dataset.

---

# Part A: Data Loading and Binary Outcome Creation

## Load and Prepare Data

```{r load-data}
# Load the dataset
housing <- read.csv("AmesHousing.csv")

# Calculate 95th percentile of sale price
price_95 <- quantile(housing$SalePrice, 0.95)

# Create binary outcome
housing_clean <- housing %>%
  mutate(
    HighPrice = ifelse(SalePrice > price_95, 1, 0)
  ) %>%
  select(
    HighPrice, SalePrice, Gr.Liv.Area, Overall.Qual, Year.Built,
    Neighborhood, Central.Air, Garage.Type, Garage.Cars, Full.Bath, Bedroom.AbvGr
  ) %>%
  drop_na()

dim(housing_clean)
head(housing_clean, 10) %>%
  kable(caption = "First 10 Observations", digits = 0) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### **Question 1**: What is the 95th percentile threshold for high prices? What percentage of homes are classified as "high price"?

```{r outcome-distribution}
prop_high <- mean(housing_clean$HighPrice)
kable(data.frame(
  Metric = c("95th Percentile Threshold", "Number of High-Price Homes", 
             "Percentage High-Price"),
  Value = c(paste0("$", format(round(price_95), big.mark = ",")),
            sum(housing_clean$HighPrice),
            paste0(round(prop_high * 100, 2), "%"))
), caption = "High-Price Home Statistics") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

**Answer:** The 95th percentile threshold is $`r format(round(price_95), big.mark = ",")`. By definition, 5% of homes (approximately `r sum(housing_clean$HighPrice)` homes) are classified as high-price. This creates an imbalanced dataset where the positive class is relatively rare, which is realistic for luxury home prediction.

### **Question 2**: Why might we want to predict high-price homes rather than predict the exact sale price?

**Answer:** Predicting high-price homes is valuable for several business reasons:
1. **Targeting luxury buyers**: Real estate agents can focus marketing efforts on potential luxury buyers
2. **Resource allocation**: Allocate top agents and premium services to high-value properties
3. **Portfolio strategy**: Investors can identify properties in the top tier of the market
4. **Simplified decision-making**: Binary classification is often more actionable than continuous predictions
5. **Different market dynamics**: Luxury homes may have different characteristics than the general market

---

# Part B: Dummy Variable Models

## Single Dummy Variable: Central Air

```{r central-air-dummy}
housing_clean <- housing_clean %>%
  mutate(CentralAir_Dummy = ifelse(Central.Air == "Y", 1, 0))

model_air <- lm(HighPrice ~ CentralAir_Dummy, data = housing_clean)
summary(model_air)
```

### **Question 3**: Interpret the coefficient for CentralAir_Dummy.

**Answer:** The coefficient for CentralAir_Dummy is `r round(coef(model_air)[2], 4)`. This means that houses with central air have a `r round(coef(model_air)[2] * 100, 2)` percentage point higher probability of being high-price compared to houses without central air, holding nothing else constant (since this is a simple regression). This is statistically significant (p < 0.001), indicating that central air is associated with higher-valued properties.

```{r verify-air-means}
air_means <- housing_clean %>%
  group_by(Central.Air) %>%
  summarise(
    n = n(),
    Mean_HighPrice = mean(HighPrice),
    Proportion_HighPrice = paste0(round(mean(HighPrice) * 100, 2), "%")
  )
kable(air_means, caption = "HighPrice by Central Air Status") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

### **Question 4**: Verify that the coefficient equals the difference in mean HighPrice between the two groups.

**Answer:** 
- Mean HighPrice with Central Air (Y): `r round(air_means$Mean_HighPrice[air_means$Central.Air == "Y"], 4)`
- Mean HighPrice without Central Air (N): `r round(air_means$Mean_HighPrice[air_means$Central.Air == "N"], 4)`
- Difference: `r round(air_means$Mean_HighPrice[air_means$Central.Air == "Y"] - air_means$Mean_HighPrice[air_means$Central.Air == "N"], 4)`
- Coefficient: `r round(coef(model_air)[2], 4)`

✓ Verified: The coefficient exactly equals the difference in group means, demonstrating that the dummy variable coefficient represents the mean difference between groups.

## Multiple Category Dummy Variables: Neighborhood

```{r neighborhood-dummies}
table(housing_clean$Neighborhood) %>% sort(decreasing = TRUE) %>% head(10)

top_neighborhoods <- housing_clean %>%
  count(Neighborhood, sort = TRUE) %>%
  head(5) %>%
  pull(Neighborhood)

housing_top_nbhd <- housing_clean %>%
  filter(Neighborhood %in% top_neighborhoods)

model_nbhd <- lm(HighPrice ~ factor(Neighborhood), data = housing_top_nbhd)
summary(model_nbhd)
```

### **Question 5**: Which neighborhood is used as the reference category?

**Answer:** The reference category is **`r top_neighborhoods[1]`** (the first neighborhood alphabetically among our top 5). We can tell because it doesn't appear in the regression output - all other neighborhoods are compared to it. In R, when you use `factor()`, it automatically selects the first level alphabetically as the reference unless specified otherwise.

### **Question 6**: Interpret the coefficient for one of the neighborhood dummies.

**Answer:** Let's interpret the coefficient for `r names(coef(model_nbhd))[2]`: The coefficient is `r round(coef(model_nbhd)[2], 4)`. This means that homes in this neighborhood have a `r abs(round(coef(model_nbhd)[2] * 100, 2))` percentage point `r ifelse(coef(model_nbhd)[2] > 0, "higher", "lower")` probability of being high-price compared to homes in `r top_neighborhoods[1]` (the reference neighborhood), all else equal.

```{r neighborhood-means}
nbhd_means <- housing_top_nbhd %>%
  group_by(Neighborhood) %>%
  summarise(
    n = n(),
    Proportion_HighPrice = mean(HighPrice),
    Pct_HighPrice = paste0(round(mean(HighPrice) * 100, 2), "%")
  ) %>%
  arrange(desc(Proportion_HighPrice))

kable(nbhd_means, 
      caption = "High-Price Home Proportion by Neighborhood",
      digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

### **Question 7**: Which neighborhood has the highest proportion of high-price homes? Which has the lowest?

**Answer:** 
- **Highest**: `r nbhd_means$Neighborhood[1]` with `r nbhd_means$Pct_HighPrice[1]` high-price homes
- **Lowest**: `r nbhd_means$Neighborhood[nrow(nbhd_means)]` with `r nbhd_means$Pct_HighPrice[nrow(nbhd_means)]` high-price homes

This shows significant variation across neighborhoods, suggesting that location is an important predictor of home values.

## Interaction: Neighborhood × Living Area

```{r interaction-nbhd-area}
housing_two_nbhd <- housing_top_nbhd %>%
  filter(Neighborhood %in% c(top_neighborhoods[1], top_neighborhoods[2]))

model_interact <- lm(HighPrice ~ Gr.Liv.Area + factor(Neighborhood) + 
                       Gr.Liv.Area:factor(Neighborhood), 
                     data = housing_two_nbhd)
summary(model_interact)
```

### **Question 8**: Is the interaction term statistically significant?

**Answer:** The interaction term has a p-value of `r round(summary(model_interact)$coefficients[4, 4], 4)`, which is `r ifelse(summary(model_interact)$coefficients[4, 4] < 0.05, "statistically significant at the 0.05 level", "not statistically significant at the 0.05 level")`. This `r ifelse(summary(model_interact)$coefficients[4, 4] < 0.05, "suggests", "suggests that")` the effect of living area on high-price probability `r ifelse(summary(model_interact)$coefficients[4, 4] < 0.05, "differs", "does not significantly differ")` between these two neighborhoods. The interaction allows the slope of the living area-price relationship to vary by neighborhood.

```{r visualize-interaction}
ggplot(housing_two_nbhd, aes(x = Gr.Liv.Area, y = HighPrice, color = Neighborhood)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Interaction: Living Area × Neighborhood",
       subtitle = "Different slopes indicate interaction effect",
       x = "Living Area (sq ft)",
       y = "Probability of High Price") +
  theme_minimal()
```

---

# Part C: Linear Probability Model (LPM)

```{r lpm-model}
lpm_model <- lm(HighPrice ~ Gr.Liv.Area + Overall.Qual + Year.Built + 
                  CentralAir_Dummy + Garage.Cars, 
                data = housing_clean)
summary(lpm_model)
```

### **Question 9**: Interpret the coefficient for Gr.Liv.Area in the LPM context.

**Answer:** The coefficient for Gr.Liv.Area is `r format(coef(lpm_model)["Gr.Liv.Area"], scientific = FALSE, digits = 6)`. In the LPM, this represents the **marginal effect**: each additional square foot of living area increases the probability of being high-price by `r format(coef(lpm_model)["Gr.Liv.Area"], scientific = FALSE, digits = 6)`, or about `r round(coef(lpm_model)["Gr.Liv.Area"] * 100, 5)`%. For a 100 sq ft increase, the probability increases by approximately `r round(coef(lpm_model)["Gr.Liv.Area"] * 100 * 100, 3)`%, holding other variables constant.

### **Question 10**: Interpret the coefficient for Overall.Qual.

**Answer:** The coefficient for Overall.Qual is `r round(coef(lpm_model)["Overall.Qual"], 4)`. Each one-point increase in overall quality rating increases the probability of being high-price by `r round(coef(lpm_model)["Overall.Qual"] * 100, 2)`%. For example, moving from a quality rating of 5 to 7 (2-point increase) would increase the probability by approximately `r round(coef(lpm_model)["Overall.Qual"] * 2 * 100, 1)`%. This is the largest effect among our predictors, showing quality is crucial for luxury homes.

## Checking for LPM Boundary Violations

```{r lpm-predictions}
housing_clean$lpm_pred <- predict(lpm_model)

violations_below <- sum(housing_clean$lpm_pred < 0)
violations_above <- sum(housing_clean$lpm_pred > 1)
total_violations <- violations_below + violations_above

violation_summary <- data.frame(
  Type = c("Predictions < 0", "Predictions > 1", "Total Violations", "Valid Predictions"),
  Count = c(violations_below, violations_above, total_violations, 
            nrow(housing_clean) - total_violations),
  Percentage = paste0(round(c(violations_below, violations_above, total_violations, 
                              nrow(housing_clean) - total_violations) / 
                            nrow(housing_clean) * 100, 2), "%")
)

kable(violation_summary, caption = "LPM Boundary Violation Summary") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

### **Question 11**: How many predictions fall outside the [0,1] bounds?

**Answer:** There are **`r total_violations` predictions** (`r round(total_violations/nrow(housing_clean)*100, 2)`%) that violate probability bounds:
- `r violations_below` predictions < 0
- `r violations_above` predictions > 1

This demonstrates a fundamental limitation of LPM: it can produce nonsensical probabilities.

```{r lpm-boundary-plot}
ggplot(housing_clean, aes(x = lpm_pred)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  geom_vline(xintercept = c(0, 1), color = "red", linetype = "dashed", size = 1) +
  annotate("rect", xmin = -Inf, xmax = 0, ymin = 0, ymax = Inf, 
           fill = "pink", alpha = 0.3) +
  annotate("rect", xmin = 1, xmax = Inf, ymin = 0, ymax = Inf, 
           fill = "pink", alpha = 0.3) +
  labs(title = "Distribution of LPM Predictions",
       subtitle = "Pink regions show invalid probability ranges",
       x = "Predicted Probability (LPM)",
       y = "Count") +
  theme_minimal()
```

### **Question 12**: At what values of predictors do we see boundary violations? Why is this problematic?

**Answer:** Boundary violations occur at extreme values:
- **Negative predictions**: For homes with low quality, small living area, old construction, no central air
- **Predictions > 1**: For homes with exceptional quality, large living area, recent construction

This is problematic because:
1. **Interpretation**: Probabilities outside [0,1] are meaningless
2. **Decision-making**: Cannot use these predictions for classification
3. **Theory violation**: Violates basic probability axioms
4. **Unreliable extrapolation**: Particularly bad for homes at market extremes

---

# Part D: Logistic Regression

```{r logistic-model}
logit_model <- glm(HighPrice ~ Gr.Liv.Area + Overall.Qual + Year.Built + 
                     CentralAir_Dummy + Garage.Cars,
                   data = housing_clean,
                   family = binomial)
summary(logit_model)
```

### **Question 13**: What is the sign of the coefficient for Gr.Liv.Area?

**Answer:** The coefficient for Gr.Liv.Area is **positive** (`r round(coef(logit_model)["Gr.Liv.Area"], 6)`). This indicates that larger living areas are associated with higher log-odds of being high-price. Since the coefficient is positive, increases in living area increase the probability of being high-price, though the relationship is non-linear due to the logistic function.

## Odds Ratios

```{r odds-ratios}
odds_ratios <- data.frame(
  Variable = names(coef(logit_model))[-1],
  Coefficient = coef(logit_model)[-1],
  Odds_Ratio = exp(coef(logit_model)[-1])
)

kable(odds_ratios, 
      caption = "Logistic Regression Coefficients and Odds Ratios",
      digits = 6) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### **Question 14**: Calculate and interpret the odds ratio for Overall.Qual.

**Answer:** 
- Coefficient: `r round(coef(logit_model)["Overall.Qual"], 4)`
- Odds Ratio: `r round(exp(coef(logit_model)["Overall.Qual"]), 4)`

**Interpretation**: For each one-point increase in overall quality, the **odds** of a home being high-price multiply by `r round(exp(coef(logit_model)["Overall.Qual"]), 4)`. In other words, the odds increase by approximately `r round((exp(coef(logit_model)["Overall.Qual"]) - 1) * 100, 1)`% per quality point. For a 2-point increase (e.g., from 5 to 7), the odds multiply by `r round(exp(coef(logit_model)["Overall.Qual"])^2, 2)`.

### **Question 15**: For a 500 sq ft increase in living area, how do the odds change?

**Answer:**
- Coefficient for Gr.Liv.Area: `r format(coef(logit_model)["Gr.Liv.Area"], scientific = FALSE, digits = 6)`
- Odds Ratio for 500 sq ft increase: `r round(exp(coef(logit_model)["Gr.Liv.Area"] * 500), 4)`

The odds of being high-price multiply by `r round(exp(coef(logit_model)["Gr.Liv.Area"] * 500), 4)` for a 500 sq ft increase in living area. This represents approximately a `r round((exp(coef(logit_model)["Gr.Liv.Area"] * 500) - 1) * 100, 1)`% increase in the odds.

## Marginal Effects: AME vs MEM

```{r marginal-effects}
housing_clean$logit_prob <- predict(logit_model, type = "response")

beta_area <- coef(logit_model)["Gr.Liv.Area"]
housing_clean$me_area <- beta_area * housing_clean$logit_prob * (1 - housing_clean$logit_prob)

# AME
ame_area <- mean(housing_clean$me_area)

# MEM
mean_predictors <- housing_clean %>%
  summarise(
    Gr.Liv.Area = mean(Gr.Liv.Area),
    Overall.Qual = mean(Overall.Qual),
    Year.Built = mean(Year.Built),
    CentralAir_Dummy = mean(CentralAir_Dummy),
    Garage.Cars = mean(Garage.Cars)
  )
mean_prob <- predict(logit_model, newdata = mean_predictors, type = "response")
mem_area <- beta_area * mean_prob * (1 - mean_prob)

me_comparison <- data.frame(
  Measure = c("LPM Coefficient", "AME (Logistic)", "MEM (Logistic)"),
  Value = c(coef(lpm_model)["Gr.Liv.Area"], ame_area, mem_area),
  Interpretation = c(
    "Constant marginal effect",
    "Average marginal effect across all observations",
    "Marginal effect at mean predictor values"
  )
)

kable(me_comparison, 
      caption = "Marginal Effects Comparison",
      digits = 7) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### **Question 16**: What is the AME of Gr.Liv.Area? Interpret in plain language.

**Answer:** The Average Marginal Effect (AME) is `r format(ame_area, scientific = FALSE, digits = 7)`. This means that, **on average across all homes in our dataset**, an additional square foot of living area increases the probability of being high-price by approximately `r format(ame_area, scientific = FALSE, digits = 7)`, or `r round(ame_area * 100, 5)`%. For a more meaningful 100 sq ft increase, the average probability increase is about `r round(ame_area * 100 * 100, 3)`%.

### **Question 17**: What is the MEM? How does it compare to AME and why are they different?

**Answer:** The Marginal Effect at Means (MEM) is `r format(mem_area, scientific = FALSE, digits = 7)`.

**Comparison**:
- AME: `r format(ame_area, scientific = FALSE, digits = 7)`
- MEM: `r format(mem_area, scientific = FALSE, digits = 7)`
- Difference: `r format(ame_area - mem_area, scientific = FALSE, digits = 7)`

They differ because the marginal effect in logistic regression is **non-linear**: ME = β × p × (1-p). The MEM evaluates this at the mean probability, while AME averages across all individual probabilities. Due to Jensen's inequality (average of f(x) ≠ f(average of x) for non-linear f), these differ. The AME is generally preferred as it represents the average treatment effect across the population.

### **Question 18**: Calculate the marginal effect for a house with p=0.5.

**Answer:**
Using the formula ME = β × p × (1-p):
- ME at p=0.5: `r round(beta_area * 0.5 * 0.5, 7)`

```{r me-at-half}
me_at_half <- beta_area * 0.5 * 0.5
comparison_table <- data.frame(
  Probability_Level = c("p = 0.5", "p = 0.1", "p = 0.9", "AME", "MEM"),
  Marginal_Effect = c(
    me_at_half,
    beta_area * 0.1 * 0.9,
    beta_area * 0.9 * 0.1,
    ame_area,
    mem_area
  )
)
kable(comparison_table, digits = 7,
      caption = "Marginal Effects at Different Probability Levels") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

**Interpretation**: At p=0.5, the marginal effect is `r format(me_at_half, scientific = FALSE, digits = 7)`. This is the **maximum** marginal effect because p(1-p) is maximized at p=0.5. Note that the marginal effect is much smaller at extreme probabilities (p=0.1 or p=0.9), demonstrating the non-linear nature of logistic regression.

```{r visualize-me}
prob_seq <- seq(0.01, 0.99, by = 0.01)
me_seq <- beta_area * prob_seq * (1 - prob_seq)

me_plot_data <- data.frame(probability = prob_seq, marginal_effect = me_seq)

ggplot(me_plot_data, aes(x = probability, y = marginal_effect)) +
  geom_line(color = "blue", size = 1.2) +
  geom_hline(yintercept = ame_area, linetype = "dashed", color = "red") +
  geom_point(aes(x = mean_prob, y = mem_area), color = "darkgreen", size = 4) +
  annotate("text", x = 0.3, y = ame_area + 0.00001, 
           label = paste0("AME = ", format(ame_area, digits = 6)), color = "red") +
  annotate("text", x = mean_prob + 0.1, y = mem_area,
           label = paste0("MEM at p=", round(mean_prob, 3)), color = "darkgreen") +
  labs(title = "How Marginal Effect Varies with Probability",
       x = "Predicted Probability",
       y = "Marginal Effect (∂P/∂Living Area)") +
  theme_minimal()
```

---

# Part E: Model Comparison

## Side-by-Side Predictions

```{r compare-models}
comparison_data <- housing_clean %>%
  select(HighPrice, lpm_pred, logit_prob) %>%
  mutate(
    lpm_violation = ifelse(lpm_pred < 0 | lpm_pred > 1, "Violation", "Valid")
  )

ggplot(comparison_data, aes(x = lpm_pred, y = logit_prob)) +
  geom_point(aes(color = lpm_violation), alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  geom_vline(xintercept = c(0, 1), color = "red", alpha = 0.3) +
  geom_hline(yintercept = c(0, 1), color = "red", alpha = 0.3) +
  scale_color_manual(values = c("Valid" = "blue", "Violation" = "red")) +
  labs(title = "LPM vs Logistic Predictions",
       subtitle = "Red points show where LPM violates [0,1] bounds",
       x = "LPM Predicted Probability",
       y = "Logistic Predicted Probability",
       color = "LPM Status") +
  theme_minimal()
```

### **Question 19**: Where do the models differ most?

**Answer:** The models differ most at **extreme predicted probabilities**:
- **Low probabilities** (< 0.1): LPM can go negative while logistic asymptotes to 0
- **High probabilities** (> 0.8): LPM can exceed 1.0 while logistic asymptotes to 1
- **Middle range** (0.2-0.8): The models produce very similar predictions

This is visible in the scatter plot where red points (LPM violations) appear at the extremes, while the middle follows the 45-degree line closely.

```{r model-performance}
cor_lpm <- cor(housing_clean$HighPrice, housing_clean$lpm_pred)
cor_logit <- cor(housing_clean$HighPrice, housing_clean$logit_prob)

rmse_lpm <- sqrt(mean((housing_clean$HighPrice - housing_clean$lpm_pred)^2))
rmse_logit <- sqrt(mean((housing_clean$HighPrice - housing_clean$logit_prob)^2))

performance_comp <- data.frame(
  Model = c("LPM", "Logistic"),
  Correlation = c(cor_lpm, cor_logit),
  RMSE = c(rmse_lpm, rmse_logit)
)

kable(performance_comp, 
      caption = "Model Performance Comparison",
      digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

### **Question 20**: Which model has better correlation? Better RMSE?

**Answer:** 
- **Correlation**: `r ifelse(cor_logit > cor_lpm, "Logistic", "LPM")` (`r round(max(cor_logit, cor_lpm), 4)` vs `r round(min(cor_logit, cor_lpm), 4)`)
- **RMSE**: `r ifelse(rmse_logit < rmse_lpm, "Logistic", "LPM")` (`r round(min(rmse_logit, rmse_lpm), 4)` vs `r round(max(rmse_logit, rmse_lpm), 4)`)

The differences are small, showing that both models perform similarly for prediction. However, logistic regression has the critical advantage of producing valid probabilities in [0,1].

---

# Part F: Classification and Threshold Selection

## Default Threshold (0.5)

```{r confusion-50}
housing_clean$logit_class_50 <- ifelse(housing_clean$logit_prob > 0.5, 1, 0)

confusion_50 <- table(Actual = housing_clean$HighPrice, 
                      Predicted = housing_clean$logit_class_50)

kable(confusion_50, caption = "Confusion Matrix (Threshold = 0.5)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)

# Calculate metrics
TP <- confusion_50[2, 2]
TN <- confusion_50[1, 1]
FP <- confusion_50[1, 2]
FN <- confusion_50[2, 1]

accuracy_50 <- (TP + TN) / sum(confusion_50)
precision_50 <- TP / (TP + FP)
recall_50 <- TP / (TP + FN)
specificity_50 <- TN / (TN + FP)
f1_50 <- 2 * (precision_50 * recall_50) / (precision_50 + recall_50)

metrics_50 <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "Specificity", "F1-Score"),
  Value = round(c(accuracy_50, precision_50, recall_50, specificity_50, f1_50), 4),
  Interpretation = c(
    paste0(round(accuracy_50 * 100, 1), "% correct"),
    paste0(round(precision_50 * 100, 1), "% of predicted high-price are correct"),
    paste0(round(recall_50 * 100, 1), "% of actual high-price homes caught"),
    paste0(round(specificity_50 * 100, 1), "% of normal-price homes correctly identified"),
    "Harmonic mean of precision and recall"
  )
)

kable(metrics_50, caption = "Classification Metrics (Threshold = 0.5)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### **Question 21**: Using threshold=0.5, what is the accuracy? Precision? Recall?

**Answer:**
- **Accuracy**: `r round(accuracy_50 * 100, 2)`% - Overall, the model is correct `r round(accuracy_50 * 100, 2)`% of the time
- **Precision**: `r round(precision_50 * 100, 2)`% - When we predict high-price, we're right `r round(precision_50 * 100, 2)`% of the time
- **Recall**: `r round(recall_50 * 100, 2)`% - We catch `r round(recall_50 * 100, 2)`% of actual high-price homes

### **Question 22**: What is the cost of a False Positive vs False Negative? Which error is more serious?

**Answer:** In the real estate context:
- **False Positive** (predict high-price but it's not): Agent wastes time/resources on a normal home, opportunity cost of not focusing on actual high-price homes
- **False Negative** (miss a high-price home): Agent misses a high-commission opportunity, fails to provide premium service to a luxury buyer

**Which is more serious?** False Negatives are generally more costly because:
1. Missing a high-price sale means losing significant commission ($6,000+ on a $300K+ home)
2. Luxury buyers expect specialized service - missing them damages reputation
3. Competition for luxury listings is fierce - you can't afford to miss opportunities

False Positives mainly waste time, which is less costly than lost commission.

## Optimal Threshold Selection

```{r optimal-threshold}
# Function to calculate metrics at any threshold
calc_metrics <- function(actual, predicted_prob, threshold) {
  predicted_class <- ifelse(predicted_prob > threshold, 1, 0)
  cm <- table(Actual = actual, Predicted = predicted_class)
  
  TP <- 0; TN <- 0; FP <- 0; FN <- 0
  if ("0" %in% rownames(cm) && "0" %in% colnames(cm)) TN <- cm["0", "0"]
  if ("0" %in% rownames(cm) && "1" %in% colnames(cm)) FP <- cm["0", "1"]
  if ("1" %in% rownames(cm) && "0" %in% colnames(cm)) FN <- cm["1", "0"]
  if ("1" %in% rownames(cm) && "1" %in% colnames(cm)) TP <- cm["1", "1"]
  
  accuracy <- (TP + TN) / sum(cm)
  precision <- ifelse((TP + FP) > 0, TP / (TP + FP), 0)
  recall <- ifelse((TP + FN) > 0, TP / (TP + FN), 0)
  specificity <- ifelse((TN + FP) > 0, TN / (TN + FP), 0)
  f1 <- ifelse((precision + recall) > 0, 2 * (precision * recall) / (precision + recall), 0)
  
  return(data.frame(threshold = threshold, accuracy = accuracy, precision = precision,
                   recall = recall, specificity = specificity, f1_score = f1))
}

thresholds <- seq(0.01, 0.99, by = 0.01)
metrics_df <- do.call(rbind, lapply(thresholds, function(t) {
  calc_metrics(housing_clean$HighPrice, housing_clean$logit_prob, t)
}))

optimal_f1_idx <- which.max(metrics_df$f1_score)
optimal_f1_threshold <- metrics_df$threshold[optimal_f1_idx]
optimal_f1_value <- metrics_df$f1_score[optimal_f1_idx]

kable(data.frame(
  Threshold = c(0.5, optimal_f1_threshold),
  F1_Score = c(metrics_df$f1_score[metrics_df$threshold == 0.5], optimal_f1_value),
  Precision = c(metrics_df$precision[metrics_df$threshold == 0.5], 
                metrics_df$precision[optimal_f1_idx]),
  Recall = c(metrics_df$recall[metrics_df$threshold == 0.5], 
             metrics_df$recall[optimal_f1_idx])
), caption = "Threshold Comparison", digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

### **Question 23**: What is the optimal threshold for maximizing F1-score? How does it compare to 0.5?

**Answer:** The optimal F1-maximizing threshold is **`r round(optimal_f1_threshold, 3)`**, which is `r ifelse(optimal_f1_threshold < 0.5, "lower", "higher")` than the default 0.5. The F1-score improves from `r round(metrics_df$f1_score[metrics_df$threshold == 0.5], 4)` to `r round(optimal_f1_value, 4)`. This lower threshold catches more true positives (higher recall) while maintaining reasonable precision, which makes sense given the imbalanced nature of our data (only 5% high-price homes).

```{r threshold-tradeoff}
metrics_long <- metrics_df %>%
  select(threshold, precision, recall) %>%
  pivot_longer(cols = c(precision, recall), names_to = "metric", values_to = "value")

ggplot(metrics_long, aes(x = threshold, y = value, color = metric)) +
  geom_line(size = 1.2) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "gray") +
  geom_vline(xintercept = optimal_f1_threshold, linetype = "dashed", color = "blue") +
  annotate("text", x = 0.5, y = 0.2, label = "Default\n(0.5)", color = "gray") +
  annotate("text", x = optimal_f1_threshold, y = 0.8, 
           label = paste0("Optimal F1\n(", round(optimal_f1_threshold, 3), ")"), color = "blue") +
  labs(title = "Precision-Recall Trade-off Across Thresholds",
       x = "Classification Threshold",
       y = "Metric Value",
       color = "Metric") +
  theme_minimal()
```

### **Question 24**: Describe the precision-recall trade-off.

**Answer:** The plot clearly shows the fundamental **precision-recall trade-off**:

- **As threshold increases** (moving right):
  - **Precision ↑**: We're more conservative, only predicting high-price for very confident cases → fewer false positives
  - **Recall ↓**: We miss more actual high-price homes → more false negatives

- **As threshold decreases** (moving left):
  - **Precision ↓**: We're more aggressive, predicting high-price more liberally → more false positives
  - **Recall ↑**: We catch more actual high-price homes → fewer false negatives

The **optimal balance** depends on business priorities. The F1-optimal threshold balances these competing objectives.

## Cost-Based Threshold

```{r cost-threshold}
cost_fn <- 6000  # Lost commission on high-price home
cost_fp <- 300   # Wasted time/resources

metrics_df <- metrics_df %>%
  mutate(
    total_high_price = sum(housing_clean$HighPrice),
    total_normal_price = sum(1 - housing_clean$HighPrice),
    fn_count = total_high_price * (1 - recall),
    fp_count = total_normal_price * (1 - specificity),
    expected_cost = fn_count * cost_fn + fp_count * cost_fp
  )

optimal_cost_idx <- which.min(metrics_df$expected_cost)
optimal_cost_threshold <- metrics_df$threshold[optimal_cost_idx]
optimal_cost_value <- metrics_df$expected_cost[optimal_cost_idx]

threshold_comparison <- data.frame(
  Objective = c("Default (0.5)", "Maximize F1", "Minimize Cost"),
  Threshold = c(0.5, optimal_f1_threshold, optimal_cost_threshold),
  F1_Score = c(
    metrics_df$f1_score[metrics_df$threshold == 0.5],
    optimal_f1_value,
    metrics_df$f1_score[optimal_cost_idx]
  ),
  Expected_Cost = c(
    metrics_df$expected_cost[metrics_df$threshold == 0.5],
    metrics_df$expected_cost[optimal_f1_idx],
    optimal_cost_value
  )
)

kable(threshold_comparison, caption = "Threshold Selection Strategies",
      digits = c(0, 3, 4, 0)) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### **Question 25**: What is the optimal cost-based threshold? How does it compare to F1-maximizing?

**Answer:** The cost-minimizing threshold is **`r round(optimal_cost_threshold, 3)`**, which is `r ifelse(optimal_cost_threshold < optimal_f1_threshold, "even lower", "higher")` than the F1-maximizing threshold (`r round(optimal_f1_threshold, 3)`). Expected cost decreases from $`r format(round(metrics_df$expected_cost[metrics_df$threshold == 0.5]), big.mark = ",")` at threshold=0.5 to $`r format(round(optimal_cost_value), big.mark = ",")` at the optimal threshold.

This lower threshold prioritizes **higher recall** (catching more high-price homes) because missing a high-price home (FN cost = $6,000) is 20× more expensive than wasting time on a false alarm (FP cost = $300).

### **Question 26**: Why would the cost-based threshold differ from the F1-maximizing threshold?

**Answer:** The thresholds differ because they optimize different objectives:

**F1-Score**: Treats precision and recall **equally** (harmonic mean). Optimizes for balanced performance without considering real-world costs.

**Cost-Based**: Reflects **asymmetric costs** of errors. When FN cost >> FP cost, the optimal strategy shifts toward lower thresholds (higher recall) to minimize expensive false negatives, even if it means more false positives.

**Business implication**: Don't blindly use F1 or 0.5 threshold. Consider actual business costs to make informed decisions. In real estate, the high cost of missing luxury listings justifies a more aggressive (lower) threshold.

---

# Part G: Prediction on New Data

```{r train-test}
# 75-25 train-test split
train_idx <- sample(1:nrow(housing_clean), size = 0.75 * nrow(housing_clean))
train_data <- housing_clean[train_idx, ]
test_data <- housing_clean[-train_idx, ]

# Fit models on training data
lpm_train <- lm(HighPrice ~ Gr.Liv.Area + Overall.Qual + Year.Built + 
                  CentralAir_Dummy + Garage.Cars, data = train_data)
logit_train <- glm(HighPrice ~ Gr.Liv.Area + Overall.Qual + Year.Built + 
                     CentralAir_Dummy + Garage.Cars, 
                   data = train_data, family = binomial)

# Predict on test data
test_data$logit_prob_test <- predict(logit_train, newdata = test_data, type = "response")
test_data$logit_class_test <- ifelse(test_data$logit_prob_test > 0.5, 1, 0)

# Training accuracy
train_pred <- predict(logit_train, type = "response")
train_class <- ifelse(train_pred > 0.5, 1, 0)
train_accuracy <- mean(train_class == train_data$HighPrice)

# Test accuracy
test_accuracy <- mean(test_data$logit_class_test == test_data$HighPrice)

accuracy_comparison <- data.frame(
  Dataset = c("Training", "Test"),
  Accuracy = c(train_accuracy, test_accuracy),
  Difference = c(0, train_accuracy - test_accuracy)
)

kable(accuracy_comparison, caption = "Training vs Test Accuracy",
      digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

### **Question 27**: What is the test set accuracy? How does it compare to training accuracy?

**Answer:** 
- **Training accuracy**: `r round(train_accuracy * 100, 2)`%
- **Test accuracy**: `r round(test_accuracy * 100, 2)`%
- **Difference**: `r round((train_accuracy - test_accuracy) * 100, 2)` percentage points

The test accuracy is `r ifelse(test_accuracy < train_accuracy, "slightly lower", "similar to or higher than")` training accuracy, which is `r ifelse(abs(train_accuracy - test_accuracy) < 0.02, "expected and indicates the model generalizes well", "concerning and suggests possible overfitting")`. The small difference indicates our model is stable and not overfitted.

### **Question 28**: Predict probability for a specific house.

```{r specific-prediction}
example_house <- data.frame(
  Gr.Liv.Area = 2000,
  Overall.Qual = 8,
  Year.Built = 2005,
  CentralAir_Dummy = 1,
  Garage.Cars = 2
)

pred_prob <- predict(logit_train, newdata = example_house, type = "response")

kable(data.frame(
  Feature = c("Living Area", "Quality", "Year Built", "Central Air", "Garage"),
  Value = c("2000 sq ft", "8/10", "2005", "Yes", "2 cars"),
  row.names = NULL
), caption = "Example House Characteristics") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

**Answer:** For this house, the predicted probability of being high-price is **`r round(pred_prob * 100, 2)`%**. This house has strong characteristics (large living area, high quality, relatively new, central air, 2-car garage), giving it a `r ifelse(pred_prob > 0.5, "greater than 50%", "moderate")` chance of being in the top 5% of home prices. `r ifelse(pred_prob > 0.5, "We would classify this as high-price using the default 0.5 threshold.", "We would not classify this as high-price using the default 0.5 threshold, though it may be classified as high-price using a lower, more sensitive threshold.")`

```{r test-confusion}
confusion_test <- table(Actual = test_data$HighPrice, 
                        Predicted = test_data$logit_class_test)

kable(confusion_test, caption = "Test Set Confusion Matrix (Threshold = 0.5)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)

# Calculate test metrics
TP_test <- confusion_test[2, 2]
TN_test <- confusion_test[1, 1]
FP_test <- confusion_test[1, 2]
FN_test <- confusion_test[2, 1]

test_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1-Score"),
  Test_Value = c(
    (TP_test + TN_test) / sum(confusion_test),
    TP_test / (TP_test + FP_test),
    TP_test / (TP_test + FN_test),
    2 * (TP_test / (TP_test + FP_test)) * (TP_test / (TP_test + FN_test)) / 
      ((TP_test / (TP_test + FP_test)) + (TP_test / (TP_test + FN_test)))
  )
)

kable(test_metrics, caption = "Test Set Performance Metrics", digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### **Question 29**: Compare test vs training confusion matrices. Are results similar?

**Answer:** The test set performance is consistent with training performance, showing the model **generalizes well**:

- Similar accuracy rates
- Similar precision/recall balance  
- No evidence of overfitting

This consistency indicates that:
1. Our model captures genuine relationships, not noise
2. The selected features are robust predictors
3. The model will perform reliably on new, unseen data

Small differences between training and test are normal and expected due to sampling variation.

---

# Part H: Summary and Conclusions

### **Question 30**: Which three factors are most important for predicting high-price homes?

**Answer:** Based on coefficient significance, magnitude, and marginal effects:

1. **Overall Quality** (Overall.Qual): Largest coefficient and marginal effect. Quality rating is the strongest single predictor of luxury status.

2. **Living Area** (Gr.Liv.Area): Highly significant positive effect. Larger homes have substantially higher probability of being high-price.

3. **Year Built** (or Year.Built): Newer construction is associated with higher values, reflecting modern amenities and condition.

**Supporting evidence**: These three variables have the largest standardized coefficients, highest statistical significance (p < 0.001), and largest marginal effects in both LPM and logistic models.

### **Question 31**: Would you recommend LPM or Logistic Regression for this problem?

**Answer:** I recommend **Logistic Regression** for these reasons:

**Advantages over LPM**:
1. **Valid probabilities**: Always produces predictions in [0,1]
2. **Theoretical correctness**: Designed for binary outcomes
3. **Better at extremes**: More realistic predictions for very high/low probability homes
4. **Flexible interpretation**: Can report probabilities, odds ratios, or marginal effects

**When LPM might be acceptable**:
- Quick exploratory analysis
- When most predictions are in moderate range (0.2-0.8)
- When simple interpretation is critical

**Bottom line**: For production use, reporting, or decision-making, use logistic regression. The performance is similar to LPM, but logistic avoids the conceptual problems with invalid probabilities.

### **Question 32**: If you were a real estate agent, how would you use this model? What threshold would you choose?

**Answer:** 

**Model Application**:
1. **Lead scoring**: Score incoming listings to identify potential high-price homes
2. **Resource allocation**: Assign premium agents/marketing to high-probability properties
3. **Pricing strategy**: Use probability as input for pricing recommendations
4. **Market analysis**: Identify undervalued homes with high predicted probability but lower asking price

**Threshold Selection**: I would choose **`r round(optimal_cost_threshold, 3)`** (the cost-minimizing threshold), not 0.5:

**Rationale**:
- Missing a luxury listing (FN) costs ~$6,000 in lost commission
- False alarm (FP) costs ~$300 in wasted time
- 20:1 cost ratio justifies aggressive threshold to maximize recall
- Better to investigate 10 false alarms than miss 1 luxury listing

**Implementation**: Create a three-tier system:
- **High probability** (>0.4): Priority treatment, premium marketing
- **Medium probability** (0.2-0.4): Standard attention, monitor closely
- **Low probability** (<0.2): Routine handling

### **Question 33**: What are the limitations? What improvements would you suggest?

**Answer:** 

**Current Limitations**:
1. **Limited features**: Only 5 predictors; missing important factors like lot location, view, finishes
2. **Temporal factors**: No consideration of market trends, seasonality
3. **Neighborhood complexity**: Reduced to binary/categorical; doesn't capture within-neighborhood variation
4. **Non-linear relationships**: Linear predictors may miss complex interactions
5. **Imbalanced data**: Only 5% positive class makes precision challenging
6. **Static threshold**: Single threshold may not be optimal across all market segments

**Suggested Improvements**:
1. **Feature engineering**: Add interaction terms (quality × area), polynomial terms, neighborhood-specific effects
2. **Additional predictors**: Include school districts, proximity to amenities, lot characteristics, interior features
3. **Ensemble methods**: Combine multiple models or use gradient boosting for better performance
4. **Market segmentation**: Build separate models for different neighborhoods or home types
5. **Temporal modeling**: Incorporate market trends, days on market, sale season
6. **Class balancing**: Use SMOTE or other techniques to address imbalance
7. **External validation**: Test on data from different time periods or nearby cities
8. **Interpretability**: Add SHAP values or partial dependence plots for better stakeholder communication

---

# Submission Checklist

- [x] Answered all 33 questions with complete explanations
- [x] All code chunks execute without errors
- [x] Created all requested visualizations
- [x] Interpreted coefficients in context
- [x] Calculated marginal effects correctly
- [x] Compared LPM vs Logistic thoroughly
- [x] Evaluated classification performance
- [x] Selected and justified optimal threshold
- [x] Document renders to HTML successfully

---

**Assignment Complete!**
