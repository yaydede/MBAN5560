---
title: "Gradient Descent Assignment - SOLUTIONS"
subtitle: "From Optimization to Regression"
author: "MBAN5520 - Statistics"
date: today
format: 
  html:
    embed-resources: true
    code-background: true
    toc: true
    toc-depth: 3
    code-fold: false
    theme: cosmo
    fig-width: 9
    fig-height: 6
    #number-sections: true
execute:
  echo: true
  warning: false
  message: false
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
set.seed(5520)
```

# Introduction

In this assignment, you will implement gradient descent (GD) from scratch to solve two problems:

1. **Part A**: A manufacturing cost optimization problem (40 points)
2. **Part B**: Predicting energy consumption using regression (60 points)

**Total: 100 points**

---

# Part A: Manufacturing Cost Optimization

## Business Context

A manufacturing company produces electronic components. The operations manager needs to determine the **optimal production batch size** to minimize total costs. The production process involves:

- **Setup costs**: Each production run requires machine setup, which costs $500. This cost is spread across all units in the batch, so larger batches reduce the per-unit setup cost.
- **Holding costs**: Inventory storage costs increase with batch size. Larger batches mean more inventory sitting in warehouses.
- **Variable costs**: Raw materials and labor cost $15 per hundred units.
- **Fixed overhead**: Monthly fixed costs are $100.

After analysis, the cost function (in hundreds of dollars) as a function of batch size $q$ (in hundreds of units) is:

$$C(q) = \frac{500}{q} + 2q^2 + 15q + 100$$

**Your task**: Use gradient descent to find the optimal batch size that minimizes total monthly cost.

---

### **Question 1** (8 points): Implement Cost and Gradient Functions

Derive the gradient $C'(q)$ and implement both the cost function and its gradient in R.

**Derivation:**

- $\frac{d}{dq}(500/q) = -500/q^2$
- $\frac{d}{dq}(2q^2) = 4q$
- $\frac{d}{dq}(15q) = 15$
- $\frac{d}{dq}(100) = 0$

$$C'(q) = -\frac{500}{q^2} + 4q + 15$$

```{r q1-functions}
# Cost function
C_cost <- function(q) {
  500/q + 2*q^2 + 15*q + 100
}

# Gradient function  
C_gradient <- function(q) {
  -500/q^2 + 4*q + 15
}

# Test: C(5) = 500/5 + 2*25 + 15*5 + 100 = 100 + 50 + 75 + 100 = 325
C_cost(5)
```

---

### **Question 2** (8 points): Implement Gradient Descent

```{r q2-gd}
gradient_descent <- function(start_q, learning_rate, n_iterations) {
  q <- start_q
  history <- data.frame(iteration = 0:n_iterations,
                        q = numeric(n_iterations + 1),
                        cost = numeric(n_iterations + 1))
  history[1, ] <- c(0, q, C_cost(q))
  
  for (i in 1:n_iterations) {
    grad <- C_gradient(q)
    q <- q - learning_rate * grad
    history[i + 1, ] <- c(i, q, C_cost(q))
  }
  return(history)
}
```

---

### **Question 3** (6 points): Find the Optimal Batch Size

```{r q3-run}
result <- gradient_descent(start_q = 1, learning_rate = 0.01, n_iterations = 100)

# Show final result
tail(result, 1)
```

**Answers:**

a) **Optimal batch size**: `r round(tail(result$q, 1), 2)` hundred units = **`r round(tail(result$q, 1) * 100, 0)` units**

b) **Minimum total cost**: `r round(tail(result$cost, 1), 2)` hundred dollars = **$`r format(round(tail(result$cost, 1) * 100, 0), big.mark=",")`**

c) **Recommendation**: The operations manager should produce approximately **464 units per batch**. This balances the tradeoff between setup costs (which favor larger batches) and holding costs (which favor smaller batches), achieving the minimum monthly cost of approximately **$25,370**.

---

### **Question 4** (6 points): Analyze Cost Components at Optimal

```{r q4-components}
optimal_q <- tail(result$q, 1)

# Calculate each component at optimal q
setup_cost <- 500/optimal_q
holding_cost <- 2*optimal_q^2
variable_cost <- 15*optimal_q
fixed_cost <- 100

# Create summary table
cost_breakdown <- data.frame(
  Component = c("Setup", "Holding", "Variable", "Fixed", "Total"),
  Cost_Hundreds = c(setup_cost, holding_cost, variable_cost, fixed_cost, 
                    setup_cost + holding_cost + variable_cost + fixed_cost),
  Cost_Dollars = c(setup_cost, holding_cost, variable_cost, fixed_cost, 
                   setup_cost + holding_cost + variable_cost + fixed_cost) * 100
)

kable(cost_breakdown, digits = 2, caption = "Cost Breakdown at Optimal Batch Size")
```

**Answer:** The variable cost ($15q) is the largest component at approximately **$6,960**. This makes business sense because:

1. Variable costs scale linearly with production - they're the direct cost of producing each unit
2. At the optimal point, setup and holding costs are balanced against each other
3. The optimization primarily balances the tradeoff between setup (decreasing with batch size) and holding (increasing with batch size), while variable costs are unavoidable regardless of batch size

---

### **Question 5** (6 points): Learning Rate Impact

```{r q5-learning-rates}
lr_001 <- gradient_descent(start_q = 1, learning_rate = 0.001, n_iterations = 100)
lr_01 <- gradient_descent(start_q = 1, learning_rate = 0.01, n_iterations = 100)
lr_05 <- gradient_descent(start_q = 1, learning_rate = 0.05, n_iterations = 100)

# Compare final values
comparison <- data.frame(
  Learning_Rate = c(0.001, 0.01, 0.05),
  Final_q = c(tail(lr_001$q, 1), tail(lr_01$q, 1), tail(lr_05$q, 1)),
  Final_Cost = c(tail(lr_001$cost, 1), tail(lr_01$cost, 1), tail(lr_05$cost, 1))
)

kable(comparison, digits = 4, caption = "Learning Rate Comparison")

# Check at 20 iterations
data.frame(
  Learning_Rate = c(0.001, 0.01, 0.05),
  q_at_20 = c(lr_001$q[21], lr_01$q[21], lr_05$q[21]),
  cost_at_20 = c(lr_001$cost[21], lr_01$cost[21], lr_05$cost[21])
) %>% kable(digits = 4, caption = "Values at 20 Iterations")
```

**Answer:** With only 20 iterations, **learning rate 0.05** gives the best business answer because:

- α = 0.001: Too slow, still far from optimal (cost ~485 vs optimal ~254)
- α = 0.01: Closer but not there yet
- α = 0.05: Reaches near-optimal quickly

However, α = 0.05 risks overshooting on some problems. The choice depends on the tradeoff between speed and stability.

---

### **Question 6** (6 points): Verify with Analytical Solution

> **Tip:** Use R's `uniroot()` function to find where $C'(q) = 0$, or use `optimize()` to directly find the minimum of $C(q)$ over an interval.

```{r q6-verify}
# Method 1: Using uniroot() to find where C'(q) = 0
analytical_q <- uniroot(function(q) C_gradient(q), interval = c(0.1, 20))$root

# Method 2: Using optimize() to find minimum directly
# analytical_result <- optimize(C_cost, interval = c(0.1, 20))
# analytical_q <- analytical_result$minimum
analytical_cost <- C_cost(analytical_q)

# GD solution
gd_q <- tail(result$q, 1)
gd_cost <- tail(result$cost, 1)

# Compare
data.frame(
  Method = c("Analytical", "Gradient Descent"),
  Optimal_q = c(analytical_q, gd_q),
  Minimum_Cost = c(analytical_cost, gd_cost),
  Difference = c(0, gd_q - analytical_q)
) %>%
  kable(digits = 4, caption = "GD vs Analytical Solution")
```

**Answer:** The GD solution is within **`r round(abs(gd_q - analytical_q), 4)`** units of the exact answer (less than 0.01% error). This difference is **negligible for the business decision** - recommending 464 vs 464.16 units makes no practical difference. The operations manager can confidently use the GD result.

---

# Part B: Energy Consumption Prediction

## Business Context

A utility company wants to predict building energy consumption based on outdoor temperature. Understanding this relationship helps with:

- Demand forecasting for power grid management
- Setting electricity rates
- Planning for peak usage periods

```{r load-energy}
energy <- read.csv("energy_consumption.csv")
head(energy)
summary(energy)
```

Note: Energy consumption follows a U-shaped pattern - high at cold temperatures (heating), low at moderate temperatures, and high at hot temperatures (cooling). We'll model this with a quadratic regression:

$$\text{consumption} = \beta_0 + \beta_1 \cdot \text{temp} + \beta_2 \cdot \text{temp}^2$$

---

### **Question 7** (6 points): Prepare the Data

```{r q7-prepare}
# Add temperature squared
energy$temp_sq <- energy$temperature^2

# Standardize predictors (not the response)
energy$temp_scaled <- (energy$temperature - mean(energy$temperature)) / sd(energy$temperature)
energy$temp_sq_scaled <- (energy$temp_sq - mean(energy$temp_sq)) / sd(energy$temp_sq)

# Create design matrix X and response y
n <- nrow(energy)
X <- cbind(1, energy$temp_scaled, energy$temp_sq_scaled)
y <- energy$consumption

# Verify dimensions
dim(X)
length(y)
```

---

### **Question 8** (10 points): Implement RSS and Gradient Functions

**Two approaches are shown below - students can use either one.**

#### Option A: Matrix Algebra Solution

```{r q8-matrix}
# Matrix algebra approach
compute_RSS_matrix <- function(X, y, beta) {
  predictions <- X %*% beta
  residuals <- y - predictions
  sum(residuals^2)
}

compute_gradient_matrix <- function(X, y, beta) {
  predictions <- X %*% beta
  residuals <- y - predictions
  -2 * t(X) %*% residuals
}
```

#### Option B: Scalar Algebra Solution

```{r q8-scalar}
# Scalar algebra approach (no matrix multiplication)
compute_RSS_scalar <- function(X, y, beta) {
  # Compute predictions manually: y_hat = beta0 + beta1*x1 + beta2*x2
  y_hat <- beta[1] + beta[2]*X[,2] + beta[3]*X[,3]
  residuals <- y - y_hat
  sum(residuals^2)
}

compute_gradient_scalar <- function(X, y, beta) {
  # Compute predictions
  y_hat <- beta[1] + beta[2]*X[,2] + beta[3]*X[,3]
  residuals <- y - y_hat
  
  # Compute each gradient component using sum()
  grad_beta0 <- -2 * sum(residuals)
  grad_beta1 <- -2 * sum(X[,2] * residuals)
  grad_beta2 <- -2 * sum(X[,3] * residuals)
  
  c(grad_beta0, grad_beta1, grad_beta2)
}
```

#### Use Either Approach (We'll use matrix for the rest)

```{r q8-functions}
# Using matrix approach (more compact)
compute_RSS <- compute_RSS_matrix
compute_gradient <- compute_gradient_matrix

# Verify both approaches give same result
beta_test <- c(50, 5, 10)
cat("Matrix RSS:", compute_RSS_matrix(X, y, beta_test), "\n")
cat("Scalar RSS:", compute_RSS_scalar(X, y, beta_test), "\n")
cat("Matrix gradient:", compute_gradient_matrix(X, y, beta_test), "\n")
cat("Scalar gradient:", compute_gradient_scalar(X, y, beta_test), "\n")
```

---

### **Question 9** (8 points): Implement Gradient Descent for Regression

```{r q9-gd-regression}
gd_regression <- function(X, y, learning_rate, n_iterations) {
  beta <- rep(0, ncol(X))
  rss_history <- numeric(n_iterations + 1)
  rss_history[1] <- compute_RSS(X, y, beta)
  
  for (i in 1:n_iterations) {
    grad <- compute_gradient(X, y, beta)
    beta <- beta - learning_rate * grad
    rss_history[i + 1] <- compute_RSS(X, y, beta)
  }
  
  return(list(beta = beta, rss_history = rss_history))
}
```

---

### **Question 10** (6 points): Run Gradient Descent

```{r q10-run}
gd_result <- gd_regression(X, y, learning_rate = 1e-6, n_iterations = 5000)

# Coefficients
beta_gd <- gd_result$beta
names(beta_gd) <- c("Intercept", "Temperature", "Temperature_Sq")
round(beta_gd, 4)

# RSS reduction
data.frame(
  Metric = c("Initial RSS", "Final RSS", "Reduction %"),
  Value = c(round(gd_result$rss_history[1], 0), 
            round(tail(gd_result$rss_history, 1), 0),
            round((1 - tail(gd_result$rss_history, 1)/gd_result$rss_history[1]) * 100, 2))
) %>% kable(caption = "GD Results")
```

---

### **Question 11** (6 points): Compare with OLS

```{r q11-compare}
ols_model <- lm(consumption ~ temp_scaled + temp_sq_scaled, data = energy)
beta_ols <- coef(ols_model)

# Comparison table
comparison <- data.frame(
  Coefficient = c("Intercept", "Temperature", "Temperature_Sq"),
  GD = beta_gd,
  OLS = beta_ols,
  Difference = beta_gd - beta_ols
)

kable(comparison, digits = 4, caption = "GD vs OLS Coefficients")
```

**Answer:** The GD coefficients are reasonably close to OLS but not identical. To bring them closer:

1. **More iterations**: Run 10,000+ iterations instead of 5,000
2. **Better learning rate**: Use adaptive learning rates or line search
3. **Different initialization**: Start closer to the solution
4. **Smaller tolerance**: Run until gradient is very small

---

### **Question 12** (6 points): Learning Rate Sensitivity

```{r q12-lr}
lr_results <- list()
for (lr in c(1e-7, 1e-6, 1e-5)) {
  result <- gd_regression(X, y, learning_rate = lr, n_iterations = 5000)
  lr_results[[paste0("lr_", lr)]] <- result
}

# Compare
data.frame(
  Learning_Rate = c("1e-7", "1e-6", "1e-5"),
  Final_RSS = sapply(lr_results, function(r) tail(r$rss_history, 1)),
  Converged = sapply(lr_results, function(r) 
    ifelse(is.na(tail(r$rss_history, 1)) | is.infinite(tail(r$rss_history, 1)), 
           "Diverged", "Yes"))
) %>% kable(digits = 2, caption = "Learning Rate Comparison")
```

**Answer:**

- **1e-7**: Very slow convergence, needs many more iterations
- **1e-6**: Good balance of speed and stability
- **1e-5**: May converge faster but risk of instability

**Recommendation**: Use **1e-6** for this problem - it converges well without diverging.

---

### **Question 13** (6 points): Predict Energy Consumption

```{r q13-predict}
# Original means and sds for scaling
mean_temp <- mean(energy$temperature)
sd_temp <- sd(energy$temperature)
mean_temp_sq <- mean(energy$temp_sq)
sd_temp_sq <- sd(energy$temp_sq)

# New temperatures to predict
new_temps <- c(-5, 15, 30)

# Calculate predictions
new_temp_sq <- new_temps^2
new_temp_scaled <- (new_temps - mean_temp) / sd_temp
new_temp_sq_scaled <- (new_temp_sq - mean_temp_sq) / sd_temp_sq

# Prediction matrix
X_new <- cbind(1, new_temp_scaled, new_temp_sq_scaled)
predictions_gd <- X_new %*% beta_gd

predictions <- data.frame(
  Temperature = new_temps,
  Predicted_Consumption = round(as.numeric(predictions_gd), 1)
)

kable(predictions, digits = 1, caption = "Energy Consumption Predictions")
```

**Answer:** The **15°C** temperature has the lowest predicted consumption (~40-45 kWh). This makes perfect sense:

- At -5°C (cold): High consumption from **heating**
- At 15°C (moderate): **Minimal HVAC needed** - neither heating nor cooling
- At 30°C (hot): High consumption from **air conditioning**

The U-shaped pattern reflects the reality of building energy use.

---

### **Question 14** (6 points): Business Interpretation

```{r q14-business}
# Use OLS coefficients on original scale
ols_original <- lm(consumption ~ temperature + I(temperature^2), data = energy)
b <- coef(ols_original)

temp_min <- -b[2] / (2 * b[3])
consumption_min <- predict(ols_original, newdata = data.frame(temperature = temp_min))

data.frame(
  Metric = c("Temperature for Min Consumption", "Minimum Consumption"),
  Value = c(paste0(round(temp_min, 1), "°C"), 
            paste0(round(consumption_min, 1), " kWh"))
) %>% kable(caption = "Optimal Operating Point")
```

**Answer:** The building requires minimum energy at approximately **`r round(temp_min, 1)`°C** (~63°F).

**Business Applications for the Utility Company:**

1. **Demand forecasting**: Expect peak demand on very cold and very hot days
2. **Pricing strategy**: Consider time-of-use rates that reflect higher costs during temperature extremes
3. **Grid planning**: Size infrastructure for peak loads in summer (cooling) and winter (heating)
4. **Energy efficiency programs**: Target buildings in extreme climate zones for efficiency upgrades
5. **Renewable integration**: Plan solar/wind capacity knowing demand patterns

---

### **Question 15** (6 points): Final Summary

```{r q15-summary}
# Time comparison
time_gd <- system.time({
  gd_regression(X, y, learning_rate = 1e-6, n_iterations = 5000)
})

time_ols <- system.time({
  lm(consumption ~ temp_scaled + temp_sq_scaled, data = energy)
})

rss_ols <- sum(residuals(ols_model)^2)

data.frame(
  Method = c("Gradient Descent (5000 iter)", "OLS"),
  Time_seconds = c(time_gd["elapsed"], time_ols["elapsed"]),
  RSS = c(round(tail(gd_result$rss_history, 1), 1), round(rss_ols, 1))
) %>% kable(digits = 4, caption = "Method Comparison")
```

**Answer:** For this problem (200 observations, 3 parameters), **OLS is clearly better**:

- Faster (direct computation vs. iteration)
- Exact solution (no convergence uncertainty)
- No tuning required (no learning rate to choose)

**When Gradient Descent is Preferred:**

1. **Very large datasets** (millions of rows): Can use mini-batches
2. **Many features** (thousands of predictors): Matrix inversion becomes expensive
3. **Non-linear models**: Neural networks have no closed-form solution
4. **Online/streaming data**: Can update continuously as new data arrives
5. **Regularization**: Easy to add L1/L2 penalties

---

# Submission Checklist

- [x] Completed all 15 questions
- [x] All code runs without errors
- [x] Answered all interpretation questions
- [x] Connected results to business context

---

**End of Assignment Solutions**
